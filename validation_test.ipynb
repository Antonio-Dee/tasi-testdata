{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import re\n",
    "client = OpenAI(api_key='')\n",
    "\n",
    "# Load the data\n",
    "file_name = 'ist4.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "ground_truths = []\n",
    "predictions = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    system_prompt = \"Evaluate the following electrical measure observation statement. Answer with just one 'True' or 'False' statement at the beginning of the answer. \"\n",
    "    #test_string = f'Is {row[\"Voltage Measurements\"]}  {row[\"Acceptance limits\"]} ?'\n",
    "    user_prompt = f'Is {row[\"Measured Values\"]}  {row[\"Expected Values (*)\"]} ?'\n",
    "\n",
    "    # Call the OpenAI API to get the validation result\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            # Define the prompt GPT\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}                                      # Common Ground for ChatGPT\n",
    "            ],\n",
    "            max_tokens=2000\n",
    "    )\n",
    "    ground_truth = input('Enter the ground truth (True/False): ')\n",
    "    \n",
    "    # Extract the generated text from the API response\n",
    "    validation_result = response.choices[0].message.content\n",
    "\n",
    "    match = re.search(r'\\b(True|False)\\b', validation_result)\n",
    "    if match.group(0) == \"True\":\n",
    "        prediction = 'True'\n",
    "    else: \n",
    "        prediction = 'False'\n",
    "    \n",
    "    ground_truths.append(ground_truth)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "ground_truths_binary = [0 if gt == 'True' else 1 for gt in ground_truths]\n",
    "predictions_binary = [0 if pred == 'True' else 1 for pred in predictions]\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(ground_truths_binary, predictions_binary).ravel()\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(ground_truths_binary, predictions_binary)\n",
    "precision = precision_score(ground_truths_binary, predictions_binary)\n",
    "recall = recall_score(ground_truths_binary, predictions_binary)\n",
    "f1 = f1_score(ground_truths_binary, predictions_binary)\n",
    "\n",
    "# Print results\n",
    "print(f'True Positives: {tp}, False Positives: {fp}, True Negatives: {tn}, False Negatives: {fn}')\n",
    "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')\n",
    "\n",
    "# Store the results in a csv\n",
    "result_df = pd.DataFrame({\n",
    "    'File Name': [file_name],\n",
    "    'Number of Tests': [len(df)],\n",
    "    'True Positives': [tp],\n",
    "    'False Positives': [fp],\n",
    "    'True Negatives': [tn],\n",
    "    'False Negatives': [fn],\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "})\n",
    "result_df.to_csv('result_gpt4.csv', mode='a', header=False, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
